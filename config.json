{
	"glove":{
		"Comment_":"#Specifications of the glove model to use",
        	"dir":"./data/glove",
        	"corpus":"6B",
        	"vec_size":"100",
        	"db":"write-math"
    },
	"directories":{
		"Comment_":"#Directories info.",
		"source_dir":"./data/squad/",
		"target_dir":"./",
		"dir":"./",
		"plots":"./plots/"
	},
	"pre":{
		"Comments_":"Define preprocessing. Run: if the preprocess is run. Split: Split paragraph into sentences. word_count_th=number of times a word nees to appear to be counted as known",
		"run": true,
		"n_chunks": 30,
		"finetune": false,
		"data_filter": true,
		"word_count_th": 10,
		"char_count_th": 50,
		"max_question_size": 60,
		"max_paragraph_size": 400,
		"max_word_size": 20,
		"known_if_glove": true,
		"use_glove_for_unk": true
	},
	"model":{
		"Comment_":"All variables related to the model's architecture and training",
                "name": "ID123 SYMMETRIC, chars double_conv,freq 1-10k, 3:0, 6B 100+50:100, XAVIER, Adadelta",
		"is_Attention_Model": true,
		"load_checkpoint":false,
		"run": true,
		"evaluate_dev": true,
		"max_out": false,
		"forget_bias":1.0,
		"n_hidden": 100,
		"input_keep_prob":0.9,
		"share_lstm_weights":true,
		"encoder_low_freq":1.0,
		"encoder_high_freq": 10000.0,
		"encoder_learn_freq": false,
                "encoder_step_skip_size": -75.0,
                "encoder_no_cross": true,
		"full_trainable_encoder": false,
		"multi_head_size": 4,
		"attention_emb_size": 100,
		"FeedForward_Hidden_Size": 264,
		"process_emb_size": 100,
                "time_encoding": true,
		"n_pre_layer": 4,
		"n_post_layer":0,
                "char_embedding":true,
		"char_embedding_size":100,
		"char_convolution_size": 5,
		"char_out_size":50,
        	"y1_sel":"double_conv",
        	"y2_sel":"direct2"
	},
	"train":{
		"check_available_memory":  false,
		"train": true,
		"batch_size":75,
		"steps":600000,
		"steps_to_save": 3000,
		"steps_to_email":21000,
		"type": "Adadelta",
		"Adam":     {"learning_rate":2e-4, "decay_rate":0,"beta1":0.9, "beta2":0.98, "epsilon":1e-8, "WarmupSteps":7200.0, "constant_LR":true},
		"Adadelta": {"learning_rate":1.0, "decay_steps": 1000, "decay_rate": 0.99, "rho":0.95},
		"Adagrad": {"learning_rate":0.05, "initial_accumulator_value":0.1},
		"RMS": {"learning_rate":0.05, "decay":0.9, "momentum":0.1},
		"dropout_encoder": 0.90,
    "dropout_attention": 0.9,
		"dropout_FF": 0.9,
		"dropout_Relu": 1.0,
		"dropout_concat": 1.0,
		"dropout_attention_pre_softmax":1.0,
		"dropout_attention_post_softmax":1.0,
		"dropout_selector":1.0,
		"label_smoothing": 0.90,
    "moving_encoder_regularization":false,
		"xavier_initialization":true
	},
	"weights_init":{
		"W_Scaling_Variance": "NOT_IMPLEMENTED",
		"pre_trained_scaling_matrix": false
	},
	"model_options":{
		"norm_layer": false,
        	"symmetric": false,
        	"switching_model":true,
        	"word2vec_scaling": "matrix",
		"word2vec_orthonormal_scaling":true,
		"use_bias": true,
          "encoder_normalization": false
				}
}
